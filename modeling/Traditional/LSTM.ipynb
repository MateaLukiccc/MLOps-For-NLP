{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score\n",
    "from google.colab import files\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import dagshub\n",
    "import os\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, f1_score, recall_score\n",
    "from early_stopper import EarlyStopper\n",
    "from train_eval import train, evaluate\n",
    "\n",
    "dagshub.init(repo_owner='MateaLukiccc', repo_name='MLOps-For-NLP', mlflow=True)\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"placeholder\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = 'placeholder'\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = \"https://dagshub.com/MateaLukiccc/MLOps-For-NLP.mlflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "files.upload()\n",
    "df_train = pd.read_csv('preprocessed_train.csv')\n",
    "df_test = pd.read_csv('preprocessed_test.csv')\n",
    "\n",
    "X_train = df_train['Text']\n",
    "X_test = df_test['Text']\n",
    "\n",
    "y_train = df_train['Class']\n",
    "y_test = df_test['Class']\n",
    "\n",
    "# Print the shapes of the training and testing sets\n",
    "print(\"The shape of X_train is \", X_train.shape)\n",
    "print(\"The shape of X_test is \", X_test.shape)\n",
    "print(\"The shape of y_train is\", y_train.shape)\n",
    "print(\"The shape of y_test is\", y_test.shape)\n",
    "\n",
    "train_data_texts = X_train.tolist()\n",
    "test_data_texts = X_test.tolist()\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data_texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "print(\"Vocabulary Size:\", vocab_size)\n",
    "\n",
    "# Convert texts to sequences and pad them\n",
    "max_length_preprocessed = 177  # example value\n",
    "x_train = pad_sequences(tokenizer.texts_to_sequences(train_data_texts), maxlen=max_length_preprocessed)\n",
    "x_test = pad_sequences(tokenizer.texts_to_sequences(test_data_texts), maxlen=max_length_preprocessed)\n",
    "\n",
    "print(\"Training X Shape:\", x_train.shape)\n",
    "print(\"Testing X Shape:\", x_test.shape)\n",
    "\n",
    "# Labels as numpy arrays\n",
    "y_train = np.array(y_train).reshape(-1, 1)\n",
    "y_test = np.array(y_test).reshape(-1, 1)\n",
    "print(\"Training Y Shape:\", y_train.shape)\n",
    "print(\"Testing Y Shape:\", y_test.shape)\n",
    "\n",
    "# One-hot encode the labels\n",
    "num_classes = 4\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(range(num_classes))\n",
    "y_train_encoded = encoder.transform(y_train)\n",
    "y_test_encoded = encoder.transform(y_test)\n",
    "\n",
    "print(\"Training Y Shape (One-Hot Encoded):\", y_train_encoded.shape)\n",
    "print(\"Testing Y Shape (One-Hot Encoded):\", y_test_encoded.shape)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.long)\n",
    "y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.float)\n",
    "x_test_tensor = torch.tensor(x_test, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test_encoded, dtype=torch.float)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "# potencial batch sizes small batches 32 64 128 256      big batches 512 1024 2048\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_size, batch_first=True, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.rnn(embedded)\n",
    "        output = self.fc(output[:, -1, :])  # Get the output of the last time step\n",
    "        return output\n",
    "        \n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.001],\n",
    "    'optim': [optim.Adam, optim.NAdam],\n",
    "    'hidden_size': [32, 64],\n",
    "    'num_layers': [1, 2],\n",
    "    'embedding_dim': [128, 256]\n",
    "}\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "for params in grid:\n",
    "  model = RNNModel(vocab_size, params['embedding_dim'], params['hidden_size'], num_classes, params['num_layers'])\n",
    "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "  model.to(device)\n",
    "\n",
    "  # Define optimizer and loss function\n",
    "  optimizer = params['optim'](model.parameters(), lr=params['learning_rate'])\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "  # Training loop\n",
    "  N_EPOCHS = 100\n",
    "  early_stopper = EarlyStopper(patience=3, min_delta=0)\n",
    "\n",
    "  mlflow.set_experiment(\"RNN\")\n",
    "  with mlflow.start_run():\n",
    "      mlflow.log_param(\"embedding_dim\", params['embedding_dim'])\n",
    "      mlflow.log_param(\"hidden_size\", params['hidden_size'])\n",
    "      mlflow.log_param(\"optimizer\", \"Adam\" if params['optim'] == optim.Adam else \"NAdam\")\n",
    "      mlflow.log_param(\"learning_rate\", params['learning_rate'])\n",
    "      mlflow.log_param(\"num_layers\", params['num_layers'])\n",
    "      mlflow.log_param(\"epochs\", N_EPOCHS)\n",
    "\n",
    "      best_test_loss = float('inf')\n",
    "      for epoch in range(N_EPOCHS):\n",
    "          train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "          test_loss = evaluate(model, test_loader, criterion, device)\n",
    "          \n",
    "          mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "          mlflow.log_metric(\"test_loss\", test_loss, step=epoch) \n",
    "          if early_stopper.early_stop(test_loss):\n",
    "              print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "              break\n",
    "          print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Test Loss: {test_loss:.3f}')\n",
    "      \n",
    "\n",
    "      model.eval()\n",
    "      all_preds = []\n",
    "      all_labels = []      \n",
    "      with torch.no_grad():\n",
    "          for batch in test_loader:\n",
    "              text, labels = batch\n",
    "              text, labels = text.to(device), labels.to(device)\n",
    "              predictions = model(text)\n",
    "              preds = predictions.argmax(dim=1)\n",
    "              all_preds.extend(preds.cpu().numpy())\n",
    "              all_labels.extend(labels.argmax(dim=1).cpu().numpy())\n",
    "      \n",
    "      # Calculate metrics\n",
    "      conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "      accuracy = accuracy_score(all_labels, all_preds)\n",
    "      f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "      recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "      print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "      print(\"Accuracy: \", accuracy)\n",
    "      print(\"F1 Score: \", f1)\n",
    "      print(\"Recall: \", recall)\n",
    "\n",
    "      # Log metrics to MLflow\n",
    "      mlflow.log_metric(\"accuracy\", accuracy)\n",
    "      mlflow.log_metric(\"f1_score\", f1)\n",
    "      mlflow.log_metric(\"recall\", recall)\n",
    "\n",
    "      # Plot and save confusion matrix as an artifact\n",
    "      disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n",
    "      disp.plot(cmap='viridis')\n",
    "      plt.savefig(\"confusion_matrix.png\")\n",
    "      mlflow.log_artifact(\"confusion_matrix.png\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
